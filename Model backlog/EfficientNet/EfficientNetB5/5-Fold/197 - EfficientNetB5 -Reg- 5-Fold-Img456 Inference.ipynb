{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import set_random_seed\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    set_random_seed(0)\n",
    "\n",
    "seed = 0\n",
    "seed_everything(seed)\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(os.path.abspath('../input/efficientnet/efficientnet-master/efficientnet-master/'))\n",
    "from efficientnet import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples:  1928\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id_code\n",
       "0  0005cfc8afb6.png\n",
       "1  003f0afdcd15.png\n",
       "2  006efc72b638.png\n",
       "3  00836aaacf06.png\n",
       "4  009245722fa4.png"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hold_out_set = pd.read_csv('../input/aptos-data-split/hold-out.csv')\n",
    "test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "print('Number of test samples: ', test.shape[0])\n",
    "\n",
    "# Preprocecss data\n",
    "test[\"id_code\"] = test[\"id_code\"].apply(lambda x: x + \".png\")\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "HEIGHT = 456\n",
    "WIDTH = 456\n",
    "CHANNELS = 3\n",
    "TTA_STEPS = 5\n",
    "\n",
    "weights_paths = ['../input/aptos-5fold-456/effNetB5_reg_img456_fold1.h5', '../input/aptos-5fold-456/effNetB5_reg_img456_fold4.h5', \n",
    "                 '../input/aptos-5fold-456/effNetB5_reg_img456_fold2.h5', '../input/aptos-5fold-456/effNetB5_reg_img456_fold5.h5', \n",
    "                 '../input/aptos-5fold-456/effNetB5_reg_img456_fold3.h5']\n",
    "n_folds = len(weights_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "labels = ['0 - No DR', '1 - Mild', '2 - Moderate', '3 - Severe', '4 - Proliferative DR']\n",
    "def plot_confusion_matrix(train, validation, labels=labels):\n",
    "    train_labels, train_preds = train\n",
    "    validation_labels, validation_preds = validation\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(24, 7))\n",
    "    train_cnf_matrix = confusion_matrix(train_labels, train_preds)\n",
    "    validation_cnf_matrix = confusion_matrix(validation_labels, validation_preds)\n",
    "\n",
    "    train_cnf_matrix_norm = train_cnf_matrix.astype('float') / train_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    validation_cnf_matrix_norm = validation_cnf_matrix.astype('float') / validation_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    train_df_cm = pd.DataFrame(train_cnf_matrix_norm, index=labels, columns=labels)\n",
    "    validation_df_cm = pd.DataFrame(validation_cnf_matrix_norm, index=labels, columns=labels)\n",
    "\n",
    "    sns.heatmap(train_df_cm, annot=True, fmt='.2f', cmap=\"Blues\",ax=ax1).set_title('Train')\n",
    "    sns.heatmap(validation_df_cm, annot=True, fmt='.2f', cmap=sns.cubehelix_palette(8),ax=ax2).set_title('Validation')\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate_model(train, validation):\n",
    "    train_labels, train_preds = train\n",
    "    validation_labels, validation_preds = validation\n",
    "    print(\"Train        Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds, train_labels, weights='quadratic'))\n",
    "    print(\"Validation   Cohen Kappa score: %.3f\" % cohen_kappa_score(validation_preds, validation_labels, weights='quadratic'))\n",
    "    print(\"Complete set Cohen Kappa score: %.3f\" % cohen_kappa_score(np.append(train_preds, validation_preds), np.append(train_labels, validation_labels), weights='quadratic'))\n",
    "\n",
    "def classify(x):\n",
    "    if x < 0.5:\n",
    "        return 0\n",
    "    elif x < 1.5:\n",
    "        return 1\n",
    "    elif x < 2.5:\n",
    "        return 2\n",
    "    elif x < 3.5:\n",
    "        return 3\n",
    "    return 4\n",
    "\n",
    "def ensemble_preds(model_list, generator):\n",
    "    preds_ensemble = []\n",
    "    for model in model_list:\n",
    "        generator.reset()\n",
    "        preds = model.predict_generator(generator, steps=generator.n)\n",
    "        preds_ensemble.append(preds)\n",
    "\n",
    "    return np.mean(preds_ensemble, axis=0)\n",
    "\n",
    "def apply_tta(model, generator, steps=10):\n",
    "    step_size = generator.n//generator.batch_size\n",
    "    preds_tta = []\n",
    "    for i in range(steps):\n",
    "        generator.reset()\n",
    "        preds = model.predict_generator(generator, steps=step_size)\n",
    "        preds_tta.append(preds)\n",
    "\n",
    "    return np.mean(preds_tta, axis=0)\n",
    "\n",
    "def test_ensemble_preds(model_list, generator, steps=10):\n",
    "    preds_ensemble = []\n",
    "    for model in model_list:\n",
    "        preds = apply_tta(model, generator, steps=steps)\n",
    "        preds_ensemble.append(preds)\n",
    "\n",
    "    return np.mean(preds_ensemble, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-procecess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "test_base_path = '../input/aptos2019-blindness-detection/test_images/'\n",
    "test_dest_path =  'base_dir/test_images/'\n",
    "\n",
    "# Making sure directories don't exist\n",
    "if os.path.exists(test_dest_path):\n",
    "    shutil.rmtree(test_dest_path)\n",
    "    \n",
    "# Creating train, validation and test directories\n",
    "os.makedirs(test_dest_path)\n",
    "\n",
    "def crop_image(img, tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "            \n",
    "        return img\n",
    "\n",
    "def circle_crop(img):\n",
    "    img = crop_image(img)\n",
    "\n",
    "    height, width, depth = img.shape\n",
    "    largest_side = np.max((height, width))\n",
    "    img = cv2.resize(img, (largest_side, largest_side))\n",
    "\n",
    "    height, width, depth = img.shape\n",
    "\n",
    "    x = width//2\n",
    "    y = height//2\n",
    "    r = np.amin((x, y))\n",
    "\n",
    "    circle_img = np.zeros((height, width), np.uint8)\n",
    "    cv2.circle(circle_img, (x, y), int(r), 1, thickness=-1)\n",
    "    img = cv2.bitwise_and(img, img, mask=circle_img)\n",
    "    img = crop_image(img)\n",
    "\n",
    "    return img\n",
    "    \n",
    "def preprocess_image(base_path, save_path, image_id, HEIGHT, WIDTH, sigmaX=10):\n",
    "    image = cv2.imread(base_path + image_id)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = circle_crop(image)\n",
    "    image = cv2.resize(image, (HEIGHT, WIDTH))\n",
    "    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0,0), sigmaX), -4 , 128)\n",
    "    cv2.imwrite(save_path + image_id, image)\n",
    "    \n",
    "# Pre-procecss test set\n",
    "for i, image_id in enumerate(test['id_code']):\n",
    "    preprocess_image(test_base_path, test_dest_path, image_id, HEIGHT, WIDTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1928 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255, \n",
    "                           rotation_range=360,\n",
    "                           horizontal_flip=True,\n",
    "                           vertical_flip=True)\n",
    "\n",
    "test_generator=datagen.flow_from_dataframe(  \n",
    "                       dataframe=test,\n",
    "                       directory=test_dest_path,\n",
    "                       x_col=\"id_code\",\n",
    "                       batch_size=1,\n",
    "                       class_mode=None,\n",
    "                       shuffle=False,\n",
    "                       target_size=(HEIGHT, WIDTH),\n",
    "                       seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, weights_path):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = EfficientNetB5(weights=None, \n",
    "                                include_top=False,\n",
    "                                input_tensor=input_tensor)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    final_output = Dense(1, activation='linear', name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "    model.load_weights(weights_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "\n",
    "for weights_path in weights_paths:\n",
    "    model_list.append(create_model(input_shape=(HEIGHT, WIDTH, CHANNELS), weights_path=weights_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model to test set and output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "preds = test_ensemble_preds(model_list, test_generator, steps=TTA_STEPS)\n",
    "predictions = [classify(x) for x in preds]\n",
    "\n",
    "results = pd.DataFrame({'id_code':test['id_code'], 'diagnosis':predictions})\n",
    "results['id_code'] = results['id_code'].map(lambda x: str(x)[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "# Cleaning created directories\n",
    "if os.path.exists(test_dest_path):\n",
    "    shutil.rmtree(test_dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x626.4 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.subplots(sharex='col', figsize=(24, 8.7))\n",
    "sns.countplot(x=\"diagnosis\", data=results, palette=\"GnBu_d\").set_title('Test')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  0005cfc8afb6          2\n",
       "1  003f0afdcd15          3\n",
       "2  006efc72b638          2\n",
       "3  00836aaacf06          2\n",
       "4  009245722fa4          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.to_csv('submission.csv', index=False)\n",
    "display(results.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
